# ü§ñ Int√©gration Ollama LLM - Guide Complet

## ‚úÖ Modifications Effectu√©es

### 1. Infrastructure (docker-compose.yml)
- ‚úÖ Ajout du service `ollama` avec image officielle
- ‚úÖ Volume `ollama-data` pour persistance du mod√®le
- ‚úÖ Port 11434 expos√©
- ‚úÖ Backend d√©pend maintenant d'Ollama

### 2. Backend - Nouveau Service LLM
- ‚úÖ Cr√©√© `backend/app/llm_service.py`
- ‚úÖ Classe `OllamaService` pour g√©n√©rer des r√©ponses intelligentes
- ‚úÖ Prompts contextuels (avec/sans RAG)

### 3. Backend - RAG Am√©lior√©
- ‚úÖ M√©thode `search_knowledge()` avec seuil de similarit√©
- ‚úÖ Retourne score de similarit√© (0.0 √† 1.0)
- ‚úÖ Seuil par d√©faut: 0.6

### 4. Backend - Endpoint Chat Hybride
- ‚úÖ Recherche RAG d'abord
- ‚úÖ Si pertinent (>0.6): Ollama + contexte RAG
- ‚úÖ Si non pertinent: Ollama seul

### 5. Configuration
- ‚úÖ Variables d'environnement Ollama ajout√©es
- ‚úÖ `OLLAMA_BASE_URL=http://ollama:11434`
- ‚úÖ `OLLAMA_MODEL=llama3.2:3b`

### 6. Scripts
- ‚úÖ `setup-ollama.sh` pour t√©l√©charger le mod√®le

---

## üöÄ D√©ploiement

### √âtape 1: Arr√™ter les Services Actuels
```bash
cd /home/omar/myWork/safran
docker compose down
```

### √âtape 2: D√©marrer avec Ollama
```bash
# D√©marrer tous les services (y compris Ollama)
docker compose up -d

# Attendre que tous les services soient pr√™ts
docker compose ps
```

### √âtape 3: Initialiser Ollama
```bash
# T√©l√©charger le mod√®le llama3.2:3b (~2 GB)
./setup-ollama.sh
```

**Temps estim√©**: ~5-10 minutes (selon votre connexion internet)

### √âtape 4: Configurer LDAP
```bash
# Cr√©er les utilisateurs LDAP
./setup-ldap.sh
```

### √âtape 5: Tester
```bash
# V√©rifier que tout fonctionne
curl http://localhost:8000/health

# Tester Ollama directement
docker exec -it hr-ollama ollama run llama3.2:3b "Bonjour"
```

---

## üéØ Exemples de Comportement

### Exemple 1: Salutation
**Utilisateur**: "Bonjour"

**Processus**:
1. RAG: Aucune correspondance (similarity < 0.6)
2. Ollama g√©n√®re r√©ponse seul

**R√©ponse**: 
> "Bonjour ! Je suis l'assistant RH de Serini. Comment puis-je vous aider aujourd'hui ? Je peux r√©pondre √† vos questions sur les cong√©s, la paie, les avantages sociaux et bien plus encore."

**Domaine**: None

---

### Exemple 2: Question RH Pertinente
**Utilisateur** (CADRE): "Comment poser un cong√© annuel ?"

**Processus**:
1. RAG: Trouve r√©ponse (similarity = 0.85)
2. Ollama utilise le contexte RAG

**R√©ponse**:
> "Pour poser un cong√© annuel, vous devez faire votre demande via le portail RH au moins 7 jours √† l'avance. Connectez-vous au portail avec vos identifiants, acc√©dez √† la section 'Cong√©s' et remplissez le formulaire de demande en indiquant les dates souhait√©es. Votre manager recevra une notification pour validation."

**Domaine**: "cong√©s"

---

### Exemple 3: Question Hors Profil
**Utilisateur** (STAGIAIRE): "Comment d√©clarer des heures suppl√©mentaires ?"

**Processus**:
1. RAG: Aucune r√©ponse pour STAGIAIRE (similarity = 0.3)
2. Ollama g√©n√®re r√©ponse seul

**R√©ponse**:
> "En tant que stagiaire, les heures suppl√©mentaires ne s'appliquent g√©n√©ralement pas √† votre type de contrat. Cependant, si vous avez des questions sp√©cifiques sur vos horaires ou votre temps de travail, je vous recommande de contacter directement le service RH ou votre tuteur de stage."

**Domaine**: None

---

### Exemple 4: Question Non-RH
**Utilisateur**: "Quel temps fait-il aujourd'hui ?"

**Processus**:
1. RAG: Aucune correspondance
2. Ollama g√©n√®re r√©ponse seul

**R√©ponse**:
> "Je suis d√©sol√©, mais je suis un assistant RH et je ne peux r√©pondre qu'aux questions li√©es aux ressources humaines de l'entreprise Serini. Puis-je vous aider avec une question concernant les cong√©s, la paie, les avantages sociaux ou tout autre sujet RH ?"

**Domaine**: None

---

### Exemple 5: Conversation Naturelle
**Utilisateur**: "Merci beaucoup !"

**R√©ponse**:
> "Je vous en prie ! N'h√©sitez pas si vous avez d'autres questions RH. Bonne journ√©e !"

---

## üîß Configuration Avanc√©e

### Changer de Mod√®le

Pour utiliser un mod√®le diff√©rent, modifiez `.env`:

```env
# Mod√®le plus petit et rapide (1.5 GB)
OLLAMA_MODEL=llama3.2:1b

# Mod√®le plus performant (4 GB)
OLLAMA_MODEL=mistral:7b

# Mod√®le tr√®s performant (4.7 GB)
OLLAMA_MODEL=llama3.1:8b
```

Puis t√©l√©chargez le nouveau mod√®le:
```bash
docker exec hr-ollama ollama pull mistral:7b
docker compose restart backend-api
```

### Ajuster le Seuil de Similarit√©

Dans `backend/app/main.py`, ligne ~220:

```python
rag_answer, domain, similarity = rag_engine.search_knowledge(
    question=request.message,
    employee_type=current_user.employee_type,
    threshold=0.6  # Modifier ici (0.0 √† 1.0)
)
```

- **0.4-0.5**: Plus permissif (plus de r√©ponses RAG)
- **0.6-0.7**: √âquilibr√© (recommand√©)
- **0.8-0.9**: Strict (seulement r√©ponses tr√®s pertinentes)

---

## üìä Ressources Syst√®me

### Avant (Sans Ollama)
- RAM: ~2 GB
- Disque: ~3 GB

### Apr√®s (Avec Ollama + llama3.2:3b)
- RAM: ~6 GB (+4 GB)
- Disque: ~5 GB (+2 GB)

### Mod√®les Alternatifs

| Mod√®le | Taille | RAM | Qualit√© | Vitesse |
|--------|--------|-----|---------|---------|
| llama3.2:1b | 1 GB | 2 GB | ‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö°‚ö°‚ö° |
| **llama3.2:3b** | 2 GB | 4 GB | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö°‚ö° |
| mistral:7b | 4 GB | 8 GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö° |
| llama3.1:8b | 4.7 GB | 8 GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö° |

---

## üêõ D√©pannage

### Ollama ne d√©marre pas
```bash
# V√©rifier les logs
docker compose logs ollama

# Red√©marrer Ollama
docker compose restart ollama
```

### Mod√®le non t√©l√©charg√©
```bash
# V√©rifier les mod√®les install√©s
docker exec hr-ollama ollama list

# T√©l√©charger manuellement
docker exec hr-ollama ollama pull llama3.2:3b
```

### R√©ponses lentes
```bash
# Utiliser un mod√®le plus petit
docker exec hr-ollama ollama pull llama3.2:1b

# Modifier .env
OLLAMA_MODEL=llama3.2:1b

# Red√©marrer backend
docker compose restart backend-api
```

### Backend ne peut pas se connecter √† Ollama
```bash
# V√©rifier le r√©seau
docker network inspect hr-chatbot-network

# V√©rifier qu'Ollama est accessible
docker exec hr-backend curl http://ollama:11434/api/tags
```

---

## ‚ú® Avantages de Cette Approche

### 1. Intelligence Contextuelle
- ‚úÖ Comprend les salutations
- ‚úÖ Peut refuser poliment les questions hors-sujet
- ‚úÖ Conversations naturelles

### 2. Pr√©cision RH
- ‚úÖ Utilise la base de connaissances quand pertinent
- ‚úÖ Cite les domaines (cong√©s, paie, etc.)
- ‚úÖ Filtre par profil utilisateur

### 3. Flexibilit√©
- ‚úÖ Peut g√©rer plusieurs tours de conversation
- ‚úÖ S'adapte au contexte
- ‚úÖ Reformule les r√©ponses de fa√ßon naturelle

### 4. Local et S√©curis√©
- ‚úÖ Pas d'API externe
- ‚úÖ Donn√©es priv√©es (RGPD)
- ‚úÖ Gratuit (pas de co√ªt API)

---

## üìù Commandes Utiles

```bash
# Tester Ollama en interactif
docker exec -it hr-ollama ollama run llama3.2:3b

# Voir les mod√®les install√©s
docker exec hr-ollama ollama list

# Supprimer un mod√®le
docker exec hr-ollama ollama rm llama3.2:3b

# Logs Ollama
docker compose logs -f ollama

# Logs Backend
docker compose logs -f backend-api

# Red√©marrer tout
docker compose restart
```

---

## üéâ R√©sultat Final

Votre chatbot RH est maintenant **intelligent** et peut:
- ‚úÖ Saluer les utilisateurs
- ‚úÖ R√©pondre aux questions RH avec pr√©cision
- ‚úÖ Refuser poliment les questions hors-sujet
- ‚úÖ Avoir des conversations naturelles
- ‚úÖ Utiliser la base de connaissances quand pertinent

**Fini les r√©ponses al√©atoires pour "bonjour" !** üéä
